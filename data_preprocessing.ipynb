{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a470a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from TransformerNetwork import * \n",
    "from t2v import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a5c3801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e503000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_data\")\n",
    "X = df.iloc[:,1:]\n",
    "y = df[\"Close\"]\n",
    "x,y = X.to_numpy(),y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4447a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(x,y, window_size):\n",
    "    xx, yy = [], []\n",
    "    for i in range(window_size,len(x)):\n",
    "        xx.append(x[i-window_size:i])\n",
    "        yy.append(y[i])\n",
    "    xx, yy = np.array(xx), np.array(yy)\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9bb2b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = sliding_window(x,y,window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "626d7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "807ab33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sinact = SineActivation(5,2)\n",
    "q = x_train[:3,:,:].astype(np.float32)\n",
    "q =torch.from_numpy(q)\n",
    "torch.reshape(q,(15,5))\n",
    "a = sinact(q)\n",
    "a.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "95a370ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5449, 0.5982, 0.5188, 0.6711, 0.1278],\n",
      "         [0.4387, 0.5035, 0.4878, 0.5961, 0.1032],\n",
      "         [0.5128, 0.5469, 0.5493, 0.5643, 0.0803],\n",
      "         [0.4739, 0.4797, 0.2901, 0.4386, 0.2245],\n",
      "         [0.3897, 0.4045, 0.4756, 0.5881, 0.0595]],\n",
      "\n",
      "        [[0.4101, 0.4287, 0.2926, 0.4373, 0.2394],\n",
      "         [0.3516, 0.3888, 0.4232, 0.5247, 0.0373],\n",
      "         [0.3850, 0.4407, 0.3995, 0.4959, 0.1150],\n",
      "         [0.4111, 0.4279, 0.3583, 0.5037, 0.1741],\n",
      "         [0.3763, 0.4416, 0.3986, 0.5023, 0.0848]],\n",
      "\n",
      "        [[0.3674, 0.4774, 0.3632, 0.5615, 0.2631],\n",
      "         [0.4920, 0.5128, 0.5357, 0.5624, 0.0860],\n",
      "         [0.4303, 0.4358, 0.4080, 0.5165, 0.0396],\n",
      "         [0.3888, 0.4615, 0.4018, 0.5249, 0.1219],\n",
      "         [0.3857, 0.3882, 0.3262, 0.4189, 0.2350]]])\n",
      "tensor([[0.5449, 0.5982, 0.5188, 0.6711, 0.1278],\n",
      "        [0.4387, 0.5035, 0.4878, 0.5961, 0.1032],\n",
      "        [0.5128, 0.5469, 0.5493, 0.5643, 0.0803],\n",
      "        [0.4739, 0.4797, 0.2901, 0.4386, 0.2245],\n",
      "        [0.3897, 0.4045, 0.4756, 0.5881, 0.0595],\n",
      "        [0.4101, 0.4287, 0.2926, 0.4373, 0.2394],\n",
      "        [0.3516, 0.3888, 0.4232, 0.5247, 0.0373],\n",
      "        [0.3850, 0.4407, 0.3995, 0.4959, 0.1150],\n",
      "        [0.4111, 0.4279, 0.3583, 0.5037, 0.1741],\n",
      "        [0.3763, 0.4416, 0.3986, 0.5023, 0.0848],\n",
      "        [0.3674, 0.4774, 0.3632, 0.5615, 0.2631],\n",
      "        [0.4920, 0.5128, 0.5357, 0.5624, 0.0860],\n",
      "        [0.4303, 0.4358, 0.4080, 0.5165, 0.0396],\n",
      "        [0.3888, 0.4615, 0.4018, 0.5249, 0.1219],\n",
      "        [0.3857, 0.3882, 0.3262, 0.4189, 0.2350]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5449, 0.5982, 0.5188, 0.6711, 0.1278],\n",
       "         [0.4387, 0.5035, 0.4878, 0.5961, 0.1032],\n",
       "         [0.5128, 0.5469, 0.5493, 0.5643, 0.0803],\n",
       "         [0.4739, 0.4797, 0.2901, 0.4386, 0.2245],\n",
       "         [0.3897, 0.4045, 0.4756, 0.5881, 0.0595]],\n",
       "\n",
       "        [[0.4101, 0.4287, 0.2926, 0.4373, 0.2394],\n",
       "         [0.3516, 0.3888, 0.4232, 0.5247, 0.0373],\n",
       "         [0.3850, 0.4407, 0.3995, 0.4959, 0.1150],\n",
       "         [0.4111, 0.4279, 0.3583, 0.5037, 0.1741],\n",
       "         [0.3763, 0.4416, 0.3986, 0.5023, 0.0848]],\n",
       "\n",
       "        [[0.3674, 0.4774, 0.3632, 0.5615, 0.2631],\n",
       "         [0.4920, 0.5128, 0.5357, 0.5624, 0.0860],\n",
       "         [0.4303, 0.4358, 0.4080, 0.5165, 0.0396],\n",
       "         [0.3888, 0.4615, 0.4018, 0.5249, 0.1219],\n",
       "         [0.3857, 0.3882, 0.3262, 0.4189, 0.2350]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = x_train[:3,:,:].astype(np.float32)\n",
    "q =torch.from_numpy(q)\n",
    "a = torch.reshape(q,(15,5))\n",
    "torch.reshape(a,(3,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8fc5146d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1783, 0.9949, 0.8106, 0.2461],\n",
       "        [0.3386, 0.3313, 0.5808, 0.2833]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,3)\n",
    "b = torch.rand(2,1)\n",
    "torch.cat((a,b),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5972f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_seq_len = window_size\n",
    "dec_seq_len = 2\n",
    "output_sequence_length = 1\n",
    "\n",
    "dim_val = 5 # input data # features\n",
    "dim_attn = 8\n",
    "lr = 0.002\n",
    "epochs = 1\n",
    "\n",
    "n_heads = 2 \n",
    "\n",
    "n_decoder_layers = 2\n",
    "n_encoder_layers = 2\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "# time to vec\n",
    "time_embed_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "83f7c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "), AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      ")]\n",
      "[AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "), AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      ")]\n",
      "[AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "), AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      ")]\n",
      "[AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "), AttentionBlock(\n",
      "  (value): Value(\n",
      "    (fc1): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      "  (key): Key(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      "  (query): Query(\n",
      "    (fc1): Linear(in_features=5, out_features=8, bias=False)\n",
      "  )\n",
      ")]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [5000, 2].  Tensor sizes: [5000, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-950202b4a6a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = Transformer(dim_val, dim_attn, batch_size, enc_seq_len, dec_seq_len, output_sequence_length, \n\u001b[0m\u001b[1;32m      2\u001b[0m                     n_decoder_layers, n_encoder_layers)\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#optimizer = torch.optim.Adam(t.parameters(), lr=lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML/ML Project/TransformerNetwork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim_val, dim_attn, input_size, dec_seq_len, out_seq_len, n_decoder_layers, n_encoder_layers, n_heads)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;31m# self.decs = nn.ModuleList()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;31m# for i in range(n_decoder_layers):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m#     self.decs.append(DecoderLayer(dim_val, dim_attn, n_heads))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML/ML Project/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, dropout, max_len)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#         pe[:, 0::2] = torch.sin(position * div_term)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;31m#         pe[:, 1::2] = torch.cos(position * div_term)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         pe = pe.unsqueeze(0).transpose(0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [5000, 2].  Tensor sizes: [5000, 3]"
     ]
    }
   ],
   "source": [
    "model = Transformer(dim_val, dim_attn, batch_size, enc_seq_len, dec_seq_len, output_sequence_length, \n",
    "                    n_decoder_layers, n_encoder_layers)\n",
    "#optimizer = torch.optim.Adam(t.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b0570e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,3,4)\n",
    "a.flatten(start_dim = 1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
